{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, ShuffleSplit, GridSearchCV\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n# import warnings \n# warnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-18T15:37:56.414902Z","iopub.execute_input":"2021-11-18T15:37:56.415344Z","iopub.status.idle":"2021-11-18T15:37:56.419815Z","shell.execute_reply.started":"2021-11-18T15:37:56.415312Z","shell.execute_reply":"2021-11-18T15:37:56.419206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1) Load and clean the data","metadata":{}},{"cell_type":"code","source":"# Load data\n# df = pd.read_csv('../input/2014-financial-data/2014_Financial_Data.csv', index_col=0)\ndf_2014 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2014_Financial_Data.csv', index_col=0)\ndf_2015 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2015_Financial_Data.csv', index_col=0)\ndf_2016 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2016_Financial_Data.csv', index_col=0)\ndf_2017 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2017_Financial_Data.csv', index_col=0)\ndf_2018 = pd.read_csv('../input/200-financial-indicators-of-us-stocks-20142018/2018_Financial_Data.csv', index_col=0)\n\ndf = df_2014.append([df_2015,df_2016,df_2017,df_2018])\nprint(df.shape)\n\n# Drop rows with no information\ndf.dropna(how='all', inplace=True)\n\n# Drop columns relative to classification\nclass_data = df.loc[:, ['Class']]\ndf.drop(['Class', '2015 PRICE VAR [%]'], inplace=True, axis=1)\n\n# Find count and percent of nan-values, zero-values\ntotal_nans = df.isnull().sum().sort_values(ascending=False)\npercent_nans = (df.isnull().sum()/df.isnull().count() * 100).sort_values(ascending=False)\ntotal_zeros = df.isin([0]).sum().sort_values(ascending=False)\npercent_zeros = (df.isin([0]).sum()/df.isin([0]).count() * 100).sort_values(ascending=False)\ndf_nans = pd.concat([total_nans, percent_nans], axis=1, keys=['Total NaN', 'Percent NaN'])\ndf_zeros = pd.concat([total_zeros, percent_zeros], axis=1, keys=['Total Zeros', 'Percent Zeros'])\n\n# Find reasonable threshold for nan-values situation\ntest_nan_level = 0.5\nprint(df_nans.quantile(test_nan_level))\n_, thresh_nan = df_nans.quantile(test_nan_level)\n\n# Find reasonable threshold for zero-values situation\ntest_zeros_level = 0.6\nprint(df_zeros.quantile(test_zeros_level))\n_, thresh_zeros = df_zeros.quantile(test_zeros_level)\n# Clean dataset applying thresholds for both zero values, nan-values\nprint(f'INITIAL NUMBER OF VARIABLES: {df.shape[1]}')\nprint()\nprint(df.shape)\n\ndf_reduce1 = df.drop((df_nans[df_nans['Percent NaN'] > thresh_nan]).index, 1)\nprint(f'NUMBER OF VARIABLES AFTER NaN THRESHOLD {thresh_nan:.2f}%: {df_reduce1.shape[1]}')\nprint()\nprint(df_reduce1.shape)\n\ndf_zeros_postnan = df_zeros.drop((df_nans[df_nans['Percent NaN'] > thresh_nan]).index, axis=0)\ndf_reduce2 = df_reduce1.drop((df_zeros_postnan[df_zeros_postnan['Percent Zeros'] > thresh_zeros]).index, 1)\nprint(f'NUMBER OF VARIABLES AFTER Zeros THRESHOLD {thresh_zeros:.2f}%: {df_reduce2.shape[1]}')\nprint(df_reduce2.shape)\n\n# Replace nan-values with mean value of column, considering each sector individually.\ndf_reduce2 = df_reduce2.groupby(['Sector']).transform(lambda x: x.fillna(x.mean()))\nprint(df_reduce2.shape)\n\n# Cut outliers\ntop_quantiles = df_reduce2.quantile(0.97)\noutliers_top = (df_reduce2 > top_quantiles)\n\nlow_quantiles = df_reduce2.quantile(0.03)\noutliers_low = (df_reduce2 < low_quantiles)\n\ndf_reduce2 = df_reduce2.mask(outliers_top, top_quantiles, axis=1)\ndf_reduce2 = df_reduce2.mask(outliers_low, low_quantiles, axis=1)\nprint(df_reduce2.shape)\n\ndf_out = df_reduce2\n# Print information about dataset\nprint(\"dataset information\")\ndf_out.info()\nprint(\"dataset describe\")\nprint(df_out.describe(include = 'all'))\nprint(\"dataset shape - now with 62 viable variales\")\nprint(df_out.shape)\nprint(class_data.shape)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:38:00.177617Z","iopub.execute_input":"2021-11-18T15:38:00.178114Z","iopub.status.idle":"2021-11-18T15:38:06.671962Z","shell.execute_reply.started":"2021-11-18T15:38:00.178076Z","shell.execute_reply":"2021-11-18T15:38:06.670996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot correlation matrix for winners \nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n\nimport seaborn as sns\ndf_out['Class'] = class_data\ndf_winners = df_out[df_out['Class'] == 1]\ndf_winners = pd.DataFrame(scaler.fit_transform(df_winners.values), columns=df_winners.columns, index=df_winners.index)\nprint(df_winners.shape)\n\nfig, ax = plt.subplots(figsize=(20,15)) \nsns.heatmap(df_winners.corr(), annot=False,cbar_kws={\"shrink\": .5},square=True, vmin=-1, vmax=1, center=0, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:38:27.731195Z","iopub.execute_input":"2021-11-18T15:38:27.731585Z","iopub.status.idle":"2021-11-18T15:38:30.507624Z","shell.execute_reply.started":"2021-11-18T15:38:27.731557Z","shell.execute_reply":"2021-11-18T15:38:30.506975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot correlation matrix for winners \ndf_losers = df_out[df_out['Class'] == 0]\nprint(df_losers.shape)\ndf_losers = pd.DataFrame(scaler.fit_transform(df_losers.values), columns=df_losers.columns, index=df_losers.index)\n\n\nfig, ax = plt.subplots(figsize=(20,15)) \nsns.heatmap(df_losers.corr(), annot=False,cbar_kws={\"shrink\": .5},square=True, vmin=-1, vmax=1, center=0, ax=ax)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:38:42.075029Z","iopub.execute_input":"2021-11-18T15:38:42.075651Z","iopub.status.idle":"2021-11-18T15:38:44.6743Z","shell.execute_reply.started":"2021-11-18T15:38:42.075612Z","shell.execute_reply":"2021-11-18T15:38:44.673409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 2) Default MultiLayer Perceptron \n\nWith and without MinMax Scaler","metadata":{}},{"cell_type":"code","source":"df_out.drop(['Class'], inplace=True, axis=1)\n\nx = df_out\ny = class_data\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)\n\nmlp = MLPClassifier(max_iter = 10000)\n\n# MinMaxScaler 0 to 1 \nprint(\"MinMax Scaler\")\nscaler = MinMaxScaler().fit(x_train)\nx_scaled_train = scaler.fit_transform(x_train)\nx_scaled_test = scaler.fit_transform(x_test)\n\nmlp.fit(x_scaled_train,y_train.values.ravel())\ny_predict_mlp = mlp.predict(x_scaled_test)\n\nprint(\"Confusion matrix\")\nprint(confusion_matrix(y_test,y_predict_mlp))\n\nprint(\"Report\")\nprint(classification_report(y_test,y_predict_mlp))\n\n# Witout scaler\nprint(\"Without scaler\")\nmlp.fit(x_train,y_train.values.ravel())\ny_predict_mlp = mlp.predict(x_test)\n\nprint(\"Confusion matrix\")\nprint(confusion_matrix(y_test,y_predict_mlp))\n\nprint(\"Report\")\nprint(classification_report(y_test,y_predict_mlp))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:38:48.315043Z","iopub.execute_input":"2021-11-18T15:38:48.315296Z","iopub.status.idle":"2021-11-18T15:40:23.915122Z","shell.execute_reply.started":"2021-11-18T15:38:48.315268Z","shell.execute_reply":"2021-11-18T15:40:23.914268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3) Tune Multilayer Perceptron","metadata":{}},{"cell_type":"code","source":"cv = ShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n\nparam_grid = {\n'activation': ['identity','logistic','tanh','relu'],\n'solver': ['lbfgs','sgd','adam'],\n'learning_rate':['constant','invscaling','adaptive']}\n\n# Gird search corss validation\ngridSearch = GridSearchCV(MLPClassifier(max_iter = 6000), param_grid, cv=cv,\n                  scoring='accuracy',verbose=2)\n\ngridSearch.fit(x_train, y_train.values.ravel())\nprint('Score: ', gridSearch.best_score_)\nprint('Parameters: ', gridSearch.best_params_)\n\n#RESULT\nprint('')","metadata":{"execution":{"iopub.status.busy":"2021-11-17T16:21:02.228739Z","iopub.execute_input":"2021-11-17T16:21:02.229264Z","iopub.status.idle":"2021-11-17T16:21:02.234265Z","shell.execute_reply.started":"2021-11-17T16:21:02.229215Z","shell.execute_reply":"2021-11-17T16:21:02.233212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tune for random_state\nX_train_s, X_tune, y_train_s, y_tune = train_test_split(x_scaled_train, y_train,test_size=0.2)\nfor i in range(15):\n    mlp = MLPClassifier(activation='identity'\n                    , solver='adam'\n                    , learning_rate='constant'\n                    , max_iter = 6000\n                    , random_state=i\n                   )\n    mlp = MLPClassifier(max_iter = 6000,random_state=i)\n\n    mlp.fit(X_train_s, y_train_s.values.ravel())\n    y_predict_mlp = mlp.predict(X_tune)\n    print(i)\n    print(classification_report(y_tune,y_predict_mlp))\n    \n# Result i = 1\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T14:57:49.496109Z","iopub.execute_input":"2021-11-18T14:57:49.496735Z","iopub.status.idle":"2021-11-18T15:00:20.583813Z","shell.execute_reply.started":"2021-11-18T14:57:49.496689Z","shell.execute_reply":"2021-11-18T15:00:20.582997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp = MLPClassifier(activation='identity'\n                    , solver='adam'\n                    , learning_rate='constant'\n                    , max_iter = 6000\n                    , random_state=1\n                   )\n\n\nprint(\"MinMax Scaler\")\nscaler = MinMaxScaler().fit(x_train)\nx_scaled_train = scaler.fit_transform(x_train)\nx_scaled_test = scaler.fit_transform(x_test)\n\nmlp.fit(x_scaled_train,y_train.values.ravel())\ny_predict_mlp = mlp.predict(x_scaled_test)\n\nprint(\"Confusion matrix\")\nprint(confusion_matrix(y_test,y_predict_mlp))\n\nprint(\"Report\")\nprint(classification_report(y_test,y_predict_mlp))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:01:38.827125Z","iopub.execute_input":"2021-11-18T15:01:38.827416Z","iopub.status.idle":"2021-11-18T15:01:46.667684Z","shell.execute_reply.started":"2021-11-18T15:01:38.827384Z","shell.execute_reply":"2021-11-18T15:01:46.666722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4) K-Nearest Neighbour ","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2)\n\n# Train the model using the training sets\nknn.fit(x_scaled_train,y_train.values.ravel())\n\n#Predict Output\ny_predict_knn = knn.predict(x_scaled_test) \nprint(\"Report\")\nprint(classification_report(y_test,y_predict_knn))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T15:40:33.274953Z","iopub.execute_input":"2021-11-18T15:40:33.275372Z","iopub.status.idle":"2021-11-18T15:40:37.916048Z","shell.execute_reply.started":"2021-11-18T15:40:33.275328Z","shell.execute_reply":"2021-11-18T15:40:37.915087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}